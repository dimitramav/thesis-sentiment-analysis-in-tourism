{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "reviews_dataset = pd.DataFrame(columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIRBNB DATASET\n",
    "\n",
    "import pandas as pd\n",
    "from utils import language_det,clean_reviews\n",
    "airbnb_dataset = pd.DataFrame(columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'])\n",
    "dfReviews = pd.DataFrame(columns=['id', 'origin','review_text', 'venue_category'])\n",
    "dfListings = pd.read_csv('airbnb/listings.csv', usecols = ['id','latitude','longitude','property_type'])\n",
    "dfReviews = pd.read_csv('airbnb/en_reviews.csv',usecols = ['id','listing_id','comments','date'])\n",
    "#dfReviews = dfReviews[dfReviews['comments'].notnull()] #keep only non Nan rows\n",
    "\n",
    "#display(dfReviews['comments'].str.len())\n",
    "#dfReviews[dfReviews['comments'] == ]\n",
    "#print(detect(dfReviews['comments'][0]))\n",
    "#dfReviews = dfReviews[dfReviews['comments'].apply(language_det) != 'en']\n",
    "\n",
    "dfReviews = dfReviews.rename(columns={\"id\": \"review_id\", \"listing_id\": \"id\"})\n",
    "dfinal = pd.merge(dfListings, dfReviews, on='id')\n",
    "dfinal['origin'] = 'airbnb'\n",
    "dfinal['type'] = 'accommodation'\n",
    "\n",
    "#dfinal.columns = ['listing_id', 'location_lat',\"location_lng\", \"venue_category\",\"id\",\"date\",\"review_text\",\"origin\",\"type\" ]\n",
    "#dfinal = dfinal.drop('listing_id', 1)\n",
    "airbnb_dataset[['id','review_text','location_lat','location_long','origin','venue_category','date','type']] = dfinal[['review_id', \n",
    "                                                                    'comments','latitude','longitude',\n",
    "                                                                    'origin','property_type','date','type']]\n",
    "airbnb_dataset = airbnb_dataset[airbnb_dataset['review_text'].notnull()] #keep only non Nan rows\n",
    "airbnb_dataset = clean_reviews(airbnb_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AIRBNB SENTIMENT ANALYSIS\n",
    "from utils import nltk_vader\n",
    "\n",
    "airbnb_list = airbnb_dataset['clean_text'].tolist()\n",
    "final_airbnb_corpus  = [\" \".join(str(word) for word in review) for review in airbnb_list]\n",
    "airbnb_dataset['sentiment'] = nltk_vader(final_airbnb_corpus)\n",
    "display(airbnb_dataset.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWITTER DATASET\n",
    "from utils import clean_reviews\n",
    "from ast import literal_eval\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "twitter_dataset = pd.DataFrame(columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'])\n",
    "\n",
    "# this finds our json files\n",
    "path_to_json_tweets = 'tweets/'\n",
    "json_tweets = [json_tweet for json_tweet in os.listdir(path_to_json_tweets)]\n",
    "tweets_pd = pd.DataFrame(columns=['id','text'])\n",
    "\n",
    "#Find tweet's location\n",
    "location_dict = {}\n",
    "with open('lexicon/twitter_location_dict') as lexicon:\n",
    "    for line in lexicon:\n",
    "        key, value = line.strip().split(\":\")\n",
    "        location_dict[key] = value\n",
    "\n",
    "reviews_list = []\n",
    "for index, js in enumerate(json_tweets):\n",
    "    with open(os.path.join(path_to_json_tweets, js)) as json_file:\n",
    "        tweet_df = json.load(json_file)\n",
    "        location = [\"37.973663\",\"23.7325461\"]\n",
    "        for word in tweet_df['text'].split():\n",
    "                if word in location_dict:\n",
    "                    location = literal_eval(location_dict[word])\n",
    "        tweet_date = time.strftime('%Y-%m-%d', time.strptime(tweet_df['created_at'],'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "        reviews_list.append({'id': tweet_df['id'], 'origin' : 'twitter','date': tweet_date,'review_text': tweet_df['text'],\n",
    "                            'location_lat':location[0],'location_long':location[1]})\n",
    "\n",
    "twitter_dataset = twitter_dataset.append(reviews_list,ignore_index=True)\n",
    "twitter_dataset = clean_reviews(twitter_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TWEETS CLASSIFICATION\n",
    "from utils import svm_classification,prepare_training_set, prepare_test_set, nltk_vader,dominant_sentiment\n",
    "\n",
    "\n",
    "#training dataset\n",
    "train_tweets_df = pd.read_csv('traindata/train2017.tsv', sep='\\t', usecols = [2,3],names = ['sentiment', 'review_text'] ,header = None)\n",
    "train_tweets_df = clean_reviews(train_tweets_df)\n",
    "final_train_corpus = prepare_training_set(train_tweets_df)\n",
    "\n",
    "#test dataset\n",
    "final_test_corpus = prepare_test_set(twitter_dataset)\n",
    "\n",
    "\n",
    "#labels\n",
    "sent_map = {\"positive\":1, \"neutral\":0, \"negative\":-1}\n",
    "y_train_tweets_labels = [sent_map[sentiment] for sentiment in train_tweets_df['sentiment'].tolist()] #sentiments\n",
    "\n",
    "#nltk vader\n",
    "nltk_vader_list = nltk_vader(final_train_corpus)\n",
    "\n",
    "#SVM AGORITHM FOR CLASSIFICATION\n",
    "bow_y_test_tweets_labels = svm_classification(final_train_corpus, final_test_corpus, y_train_tweets_labels,'BOW')\n",
    "tfidf_y_test_tweets_labels =  svm_classification(final_train_corpus, final_test_corpus, y_train_tweets_labels,'TFIDF')\n",
    "twitter_dataset['sentiment'] = dominant_sentiment(bow_y_test_tweets_labels,tfidf_y_test_tweets_labels,nltk_vader_list)\n",
    "display(twitter_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOURSQUARE DATASET\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from utils import clean_reviews\n",
    "foursquare_dataset = pd.DataFrame(columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'])\n",
    "            \n",
    "# this finds our json files\n",
    "root = 'foursquare/'\n",
    "reviews_list = []\n",
    "for path, dirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if name.endswith('.json'):\n",
    "            with open(os.path.join(path, name)) as json_file:\n",
    "                foursquare_df = json.load(json_file)\n",
    "                for tip in foursquare_df['venue']['tips']['groups'][0]['items']:  #2 most upvoted tips\n",
    "                    if('lang' in tip):\n",
    "                        if(tip['lang']=='en'):  #keep english only\n",
    "                            #print(tip['text'])\n",
    "                            foursquare_date  = datetime.utcfromtimestamp(tip['createdAt']).strftime('%Y-%m-%d')\n",
    "                            reviews_list.append({'id': foursquare_df['venue']['id'], \n",
    "                                      'origin' : 'foursquare',\n",
    "                                      'date': foursquare_date,\n",
    "                                      'review_text': tip['text'],\n",
    "                                      'location_lat':foursquare_df['venue']['location']['lat'],\n",
    "                                      'location_long':foursquare_df['venue']['location']['lng'],\n",
    "                                      'type':os.path.basename(path),\n",
    "                                      'venue_category':foursquare_df['venue']['categories'][0]['name']})\n",
    "foursquare_dataset = foursquare_dataset.append(reviews_list,ignore_index=True)\n",
    "foursquare_dataset = clean_reviews(foursquare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOURSQUARE CLASSIFICATION\n",
    "from utils import svm_classification,prepare_training_set, prepare_test_set, nltk_vader,dominant_sentiment\n",
    "\n",
    "#training dataset\n",
    "train_foursquare_df = pd.read_csv('traindata/en_foursquare_train.csv', sep=',', usecols = [0,1],names = ['review_text', 'sentiment'],skiprows=1)\n",
    "train_foursquare_df = clean_reviews(train_foursquare_df)\n",
    "final_train_corpus = prepare_training_set(train_foursquare_df)\n",
    "\n",
    "#test dataset\n",
    "final_test_corpus = prepare_test_set(foursquare_dataset)\n",
    "\n",
    "\n",
    "#labels\n",
    "y_train_foursquare_labels = train_foursquare_df['sentiment'].tolist() #sentiments\n",
    "\n",
    "#nltk vader\n",
    "nltk_vader_list = nltk_vader(final_train_corpus)\n",
    "\n",
    "bow_y_test_foursquare_labels = svm_classification(final_train_corpus, final_test_corpus, y_train_foursquare_labels,'BOW')\n",
    "tfidf_y_test_foursquare_labels =  svm_classification(final_train_corpus, final_test_corpus, y_train_foursquare_labels,'TFIDF')\n",
    "\n",
    "foursquare_dataset['sentiment'] = dominant_sentiment(bow_y_test_foursquare_labels,tfidf_y_test_foursquare_labels,nltk_vader_list)\n",
    "display(foursquare_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAPS DATASET\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "maps_dataset = pd.DataFrame(columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'])\n",
    "\n",
    "# this finds our json files\n",
    "root = 'places/'\n",
    "reviews_list = []\n",
    "for path, dirs, files in os.walk(root):\n",
    "    for name in files:\n",
    "        if name.endswith('.json'):\n",
    "            with open(os.path.join(path, name)) as json_file:\n",
    "                maps_df = json.load(json_file)\n",
    "                for review in maps_df['reviews']:  #5 most useful tips\n",
    "                    if('language' in review):\n",
    "                        if(review['language']=='en'):  #keep english only\n",
    "                            if float(maps_df['rating'])<3 and float(maps_df['rating'])>0:\n",
    "                                sentiment = '-1'\n",
    "                            elif float(maps_df['rating'])<5 and float(maps_df['rating'])>4 :\n",
    "                                sentiment = '1'\n",
    "                            else:\n",
    "                                sentiment = '0'\n",
    "                            reviews_list.append({'id': maps_df['id'], \n",
    "                                      'origin' : 'google_maps',\n",
    "                                      'review_text': review['text'],\n",
    "                                      'rating':maps_df['rating'],\n",
    "                                      'location_lat':maps_df['location']['lat'],\n",
    "                                      'location_long':maps_df['location']['lng'],\n",
    "                                      'type':os.path.basename(path),\n",
    "                                      'sentiment':sentiment,\n",
    "                                      'venue_category':maps_df['type']})\n",
    "        else:\n",
    "            #data was not valid JSON\n",
    "            continue\n",
    "\n",
    "maps_dataset = maps_dataset.append(reviews_list,ignore_index=True)\n",
    "maps_dataset = clean_reviews(maps_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from utils import format_topics_sentences,topic_keywords\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "\n",
    "#topic dictionary\n",
    "topic = []\n",
    "counter = 0\n",
    "for review in reviews_dataset['clean_text'].tolist():\n",
    "    id2word = corpora.Dictionary([review])\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in [review]]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           per_word_topics=True);\n",
    "    keywords = topic_keywords(lda_model, corpus,[review])\n",
    "    counter +=1\n",
    "    if counter % 1000 == 0:\n",
    "        print(counter)\n",
    "    topic.append(keywords.tolist())\n",
    "reviews_dataset['topic'] = topic\n",
    "reviews_dataset.to_csv(r'reviews_dataset_test.csv',columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','address','sentiment'] ,index=False) #save reviews to file for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCAT NEW DATA TO EXISTING CSV\n",
    "reviews_dataset = [maps_dataset,twitter_dataset,airbnb_dataset,foursquare_dataset]\n",
    "reviews_dataset = pd.concat(reviews_dataset,ignore_index=True)\n",
    "display(reviews_dataset[reviews_dataset['origin']=='airbnb'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONVERT COORDS TO AREAS\n",
    "\n",
    "#FIND GENERAL LOCATION USING A LIBRARY THAT EXPLOITS AN OFFLINE DATABASE\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "reviews_dataset[\"address\"] = \"\"\n",
    "llat = reviews_dataset['location_lat'].tolist()\n",
    "llng = reviews_dataset['location_long'].tolist()\n",
    "coordinates = tuple(zip(llat, llng))  \n",
    "results = rg.search(coordinates)\n",
    "cities = []\n",
    "for city in results:\n",
    "    cities.append(city.get('name'))\n",
    "reviews_dataset['address'] = cities\n",
    "\n",
    "#FIND PARTICUAR USING A WEB API (REALY SLOW PROCESS)\n",
    "#TO MONITOR THE WHOLE PROCESS I HAVE SPLIT DATAFRAME INTO CHUNKS\n",
    "from geopy.geocoders import Nominatim\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "geolocator = Nominatim(timeout = 3)\n",
    "\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "geocode = RateLimiter(geolocator.reverse, min_delay_seconds=1)\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "all_files = glob.glob('df_chunks' + \"/*.csv\")\n",
    "for filename in all_files:\n",
    "    reviews_test = pd.read_csv(filename)\n",
    "    reviews_test['address'] = reviews_test.apply(lambda row:[geocode((str(row['location_lat']) +\",\"+ str(row['location_long'])),language= 'en')] \n",
    "                                                   if row['address'] == 'Athens' else row['address'], axis=1)\n",
    "    reviews_test.to_csv(filename,columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','address','sentiment'] ,index=False) #save reviews to file for future use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESS THE RETURNED RESULTS FROM NOMINATIM\n",
    "from utils import specify_location\n",
    "reviews_dataset['address'] = reviews_dataset_coords['address'].apply(specify_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('geojson/synoikies.geojson') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for feature in data['features']:\n",
    "    print(feature['properties']['SINIKIA_EN'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCAT NEW DATA TO EXISTING CSV\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "all_files = glob.glob('df_chunks' + \"/*.csv\")\n",
    "df_list = []\n",
    "for filename in all_files:\n",
    "    df_list.append(pd.read_csv(filename))\n",
    "    reviews_dataset_coords = pd.concat(df_list,ignore_index=True)\n",
    "    reviews_dataset_coords.to_csv(r'reviews_dataset_coords.csv',columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','address','sentiment'] ,index=False) #save reviews to file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WRITE DATAFRAME TO FILE\n",
    "reviews_dataset.to_csv(r'reviews_dataset.csv',columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','sentiment'] ,index=False) #save reviews to file for future use\n",
    "#reviews_dataset_coords.to_csv(r'reviews_dataset.csv',columns=['id', 'origin','date','review_text','clean_text','rating','location_lat','location_long','venue_category','type','topic','address','sentiment'] ,index=False) #save reviews to file for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
